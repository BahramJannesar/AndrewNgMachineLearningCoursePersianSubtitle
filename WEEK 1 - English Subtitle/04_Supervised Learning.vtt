WEBVTT

1
00:00:00.000 --> 00:00:02.190
In this video, I'm going to define what is

2
00:00:02.190 --> 00:00:04.900
probably the most common type of Machine Learning problem,

3
00:00:04.900 --> 00:00:06.655
which is Supervised Learning.

4
00:00:06.655 --> 00:00:09.685
I'll define Supervised Learning more formally later,

5
00:00:09.685 --> 00:00:14.010
but it's probably best to explain or start with an example of what it is,

6
00:00:14.010 --> 00:00:16.705
and we'll do the formal definition later.

7
00:00:16.705 --> 00:00:19.915
Let's say you want to predict housing prices.

8
00:00:19.915 --> 00:00:24.755
A while back a student collected data sets from the City of Portland,

9
00:00:24.755 --> 00:00:29.060
Oregon, and let's say you plot the data set and it looks like this.

10
00:00:29.060 --> 00:00:31.000
Here on the horizontal axis,

11
00:00:31.000 --> 00:00:33.240
the size of different houses in square feet,

12
00:00:33.240 --> 00:00:34.920
and on the vertical axis,

13
00:00:34.920 --> 00:00:38.825
the price of different houses in thousands of dollars.

14
00:00:38.825 --> 00:00:41.515
So, given this data,

15
00:00:41.515 --> 00:00:47.760
let's say you have a friend who owns a house that is say 750 square feet,

16
00:00:47.760 --> 00:00:49.535
and they are hoping to sell the house,

17
00:00:49.535 --> 00:00:52.080
and they want to know how much they can get for the house.

18
00:00:52.080 --> 00:00:54.680
So, how can the learning algorithm help you?

19
00:00:54.680 --> 00:00:57.080
One thing a learning algorithm might be want to do

20
00:00:57.080 --> 00:00:59.500
is put a straight line through the data,

21
00:00:59.500 --> 00:01:02.185
also fit a straight line to the data.

22
00:01:02.185 --> 00:01:10.740
Based on that, it looks like maybe their house can be sold for maybe about $150,000.

23
00:01:10.740 --> 00:01:13.890
But maybe this isn't the only learning algorithm you can use,

24
00:01:13.890 --> 00:01:15.530
and there might be a better one.

25
00:01:15.530 --> 00:01:19.000
For example, instead of fitting a straight line to the data,

26
00:01:19.000 --> 00:01:22.030
we might decide that it's better to fit a quadratic function,

27
00:01:22.030 --> 00:01:25.300
or a second-order polynomial to this data.

28
00:01:25.300 --> 00:01:28.290
If you do that and make a prediction here,

29
00:01:28.290 --> 00:01:30.530
then it looks like, well,

30
00:01:30.530 --> 00:01:35.030
maybe they can sell the house for closer to $200,000.

31
00:01:35.030 --> 00:01:38.250
One of the things we'll talk about later is how to choose,

32
00:01:38.250 --> 00:01:41.700
and how to decide, do you want to fit a straight line to the data?

33
00:01:41.700 --> 00:01:44.360
Or do you want to fit a quadratic function to the data?

34
00:01:44.360 --> 00:01:48.755
There's no fair picking whichever one gives your friend the better house to sell.

35
00:01:48.755 --> 00:01:52.520
But each of these would be a fine example of a learning algorithm.

36
00:01:52.520 --> 00:01:57.560
So, this is an example of a Supervised Learning algorithm.

37
00:01:57.560 --> 00:02:00.890
The term Supervised Learning refers to

38
00:02:00.890 --> 00:02:04.920
the fact that we gave the algorithm a data set in which the,

39
00:02:04.920 --> 00:02:06.960
called, "right answers" were given.

40
00:02:06.960 --> 00:02:13.830
That is we gave it a data set of houses in which for every example in this data set,

41
00:02:13.830 --> 00:02:16.240
we told it what is the right price.

42
00:02:16.240 --> 00:02:19.905
So, what was the actual price that that house sold for,

43
00:02:19.905 --> 00:02:23.630
and the task of the algorithm was to just produce more of

44
00:02:23.630 --> 00:02:28.975
these right answers such as for this new house that your friend may be trying to sell.

45
00:02:28.975 --> 00:02:31.390
To define a bit more terminology,

46
00:02:31.390 --> 00:02:34.670
this is also called a regression problem.

47
00:02:34.670 --> 00:02:39.410
By regression problem, I mean we're trying to predict a continuous valued output.

48
00:02:39.410 --> 00:02:41.900
Namely the price. So technically,

49
00:02:41.900 --> 00:02:44.470
I guess prices can be rounded off to the nearest cent.

50
00:02:44.470 --> 00:02:47.260
So, maybe prices are actually discrete value.

51
00:02:47.260 --> 00:02:51.020
But usually, we think of the price of a house as a real number, as a scalar value,

52
00:02:51.020 --> 00:02:52.945
as a continuous value number,

53
00:02:52.945 --> 00:02:55.670
and the term regression refers to the fact that we're trying to

54
00:02:55.670 --> 00:02:59.000
predict the sort of continuous values attribute.

55
00:02:59.000 --> 00:03:01.770
Here's another Supervised Learning examples.

56
00:03:01.770 --> 00:03:04.030
Some friends and I were actually working on this earlier.

57
00:03:04.030 --> 00:03:06.800
Let's say you want to look at medical records and try to

58
00:03:06.800 --> 00:03:09.980
predict of a breast cancer as malignant or benign.

59
00:03:09.980 --> 00:03:12.475
If someone discovers a breast tumor,

60
00:03:12.475 --> 00:03:13.825
a lump in their breast,

61
00:03:13.825 --> 00:03:18.320
a malignant tumor is a tumor that is harmful and dangerous,

62
00:03:18.320 --> 00:03:21.830
and a benign tumor is a tumor that is harmless.

63
00:03:21.830 --> 00:03:24.405
So obviously, people care a lot about this.

64
00:03:24.405 --> 00:03:26.510
Let's see collected data set.

65
00:03:26.510 --> 00:03:28.430
Suppose you are in your dataset,

66
00:03:28.430 --> 00:03:33.170
you have on your horizontal axis the size of the tumor,

67
00:03:33.170 --> 00:03:34.640
and on the vertical axis,

68
00:03:34.640 --> 00:03:37.320
I'm going to plot one or zero, yes or no,

69
00:03:37.320 --> 00:03:41.720
whether or not these are examples of tumors we've seen before are malignant,

70
00:03:41.720 --> 00:03:45.980
which is one, or zero or not malignant or benign.

71
00:03:45.980 --> 00:03:48.470
So, let's say your dataset looks like this,

72
00:03:48.470 --> 00:03:51.645
where we saw a tumor of this size that turned out to be benign,

73
00:03:51.645 --> 00:03:52.880
one of this size,

74
00:03:52.880 --> 00:03:57.000
one of this size, and so on.

75
00:03:57.000 --> 00:03:59.730
Sadly, we also saw a few malignant tumors cell,

76
00:03:59.730 --> 00:04:00.750
one of that size,

77
00:04:00.750 --> 00:04:02.230
one of that size,

78
00:04:02.230 --> 00:04:05.540
one of that size, so on.

79
00:04:05.540 --> 00:04:11.680
So in this example, I have five examples of benign tumors shown down here,

80
00:04:11.680 --> 00:04:17.850
and five examples of malignant tumors shown with a vertical axis value of one.

81
00:04:17.850 --> 00:04:21.730
Let's say a friend who tragically has a breast tumor,

82
00:04:21.730 --> 00:04:27.725
and let's say her breast tumor size is maybe somewhere around this value,

83
00:04:27.725 --> 00:04:29.530
the Machine Learning question is,

84
00:04:29.530 --> 00:04:31.690
can you estimate what is the probability,

85
00:04:31.690 --> 00:04:35.605
what's the chance that a tumor as malignant versus benign?

86
00:04:35.605 --> 00:04:38.035
To introduce a bit more terminology,

87
00:04:38.035 --> 00:04:41.720
this is an example of a classification problem.

88
00:04:41.720 --> 00:04:45.200
The term classification refers to the fact, that here,

89
00:04:45.200 --> 00:04:51.820
we're trying to predict a discrete value output zero or one, malignant or benign.

90
00:04:51.820 --> 00:04:54.920
It turns out that in classification problems,

91
00:04:54.920 --> 00:05:00.665
sometimes you can have more than two possible values for the output.

92
00:05:00.665 --> 00:05:02.430
As a concrete example,

93
00:05:02.430 --> 00:05:05.590
maybe there are three types of breast cancers.

94
00:05:05.590 --> 00:05:09.725
So, you may try to predict a discrete value output zero, one, two,

95
00:05:09.725 --> 00:05:12.720
or three, where zero may mean benign,

96
00:05:12.720 --> 00:05:14.815
benign tumor, so no cancer,

97
00:05:14.815 --> 00:05:18.110
and one may mean type one cancer,

98
00:05:18.110 --> 00:05:19.515
maybe three types of cancer,

99
00:05:19.515 --> 00:05:23.000
whatever type one means, and two mean a second type of cancer,

100
00:05:23.000 --> 00:05:25.490
and three may mean a third type of cancer.

101
00:05:25.490 --> 00:05:28.730
But this will also be a classification problem because this are

102
00:05:28.730 --> 00:05:33.525
the discrete value set of output corresponding to you're no cancer,

103
00:05:33.525 --> 00:05:35.220
or cancer type one, or cancer type two,

104
00:05:35.220 --> 00:05:36.470
or cancer types three.

105
00:05:36.470 --> 00:05:41.255
In classification problems, there is another way to plot this data.

106
00:05:41.255 --> 00:05:43.040
Let me show you what I mean. I'm going to use

107
00:05:43.040 --> 00:05:46.440
a slightly different set of symbols to plot this data.

108
00:05:46.440 --> 00:05:49.640
So, if tumor size is going to be the attribute that I'm

109
00:05:49.640 --> 00:05:52.700
going to use to predict malignancy or benignness,

110
00:05:52.700 --> 00:05:54.500
I can also draw my data like this.

111
00:05:54.500 --> 00:05:58.220
I'm going to use different symbols to denote my benign and malignant,

112
00:05:58.220 --> 00:06:00.170
or my negative and positive examples.

113
00:06:00.170 --> 00:06:02.205
So, instead of drawing crosses,

114
00:06:02.205 --> 00:06:06.880
I'm now going to draw O's for the benign tumors,

115
00:06:07.370 --> 00:06:16.755
like so, and I'm going to keep using X's to denote my malignant tumors.

116
00:06:16.755 --> 00:06:22.050
I hope this figure makes sense. All I did was I took my data set on top,

117
00:06:22.050 --> 00:06:28.490
and I just mapped it down to this real line like so,

118
00:06:28.490 --> 00:06:30.500
and started to use different symbols,

119
00:06:30.500 --> 00:06:35.705
circles and crosses to denote malignant versus benign examples.

120
00:06:35.705 --> 00:06:37.780
Now, in this example,

121
00:06:37.780 --> 00:06:40.610
we use only one feature or one attribute,

122
00:06:40.610 --> 00:06:46.475
namely the tumor size in order to predict whether a tumor is malignant or benign.

123
00:06:46.475 --> 00:06:48.200
In other machine learning problems,

124
00:06:48.200 --> 00:06:51.250
when we have more than one feature or more than one attribute.

125
00:06:51.250 --> 00:06:55.610
Here's an example, let's say that instead of just knowing the tumor size,

126
00:06:55.610 --> 00:06:58.870
we know both the age of the patients and the tumor size.

127
00:06:58.870 --> 00:07:02.365
In that case, maybe your data set would look like this,

128
00:07:02.365 --> 00:07:06.940
where I may have a set of patients with those ages,

129
00:07:06.940 --> 00:07:08.060
and that tumor size,

130
00:07:08.060 --> 00:07:09.770
and they look like this,

131
00:07:09.770 --> 00:07:15.130
and different set of patients that look a little different,

132
00:07:15.460 --> 00:07:22.100
whose tumors turn out to be malignant as denoted by the crosses.

133
00:07:22.100 --> 00:07:27.740
So, let's say you have a friend who tragically has a tumor,

134
00:07:27.740 --> 00:07:33.910
and maybe their tumor size and age falls around there.

135
00:07:33.910 --> 00:07:36.160
So, given a data set like this,

136
00:07:36.160 --> 00:07:40.700
what the learning algorithm might do is fit a straight line to the data to

137
00:07:40.700 --> 00:07:45.380
try to separate out the malignant tumors from the benign ones,

138
00:07:45.380 --> 00:07:48.800
and so the learning algorithm may decide to put a straight line like

139
00:07:48.800 --> 00:07:52.920
that to separate out the two causes of tumors.

140
00:07:52.920 --> 00:07:58.280
With this, hopefully we can decide that your friend's tumor is more likely,

141
00:07:58.280 --> 00:07:59.960
if it's over there that hopefully

142
00:07:59.960 --> 00:08:02.795
your learning algorithm will say that your friend's tumor

143
00:08:02.795 --> 00:08:08.635
falls on this benign side and is therefore more likely to be benign than malignant.

144
00:08:08.635 --> 00:08:11.345
In this example, we had two features namely,

145
00:08:11.345 --> 00:08:14.335
the age of the patient and the size of the tumor.

146
00:08:14.335 --> 00:08:16.730
In other Machine Learning problems,

147
00:08:16.730 --> 00:08:19.100
we will often have more features.

148
00:08:19.100 --> 00:08:22.320
My friends that worked on this problem actually used other features like these,

149
00:08:22.320 --> 00:08:24.145
which is clump thickness,

150
00:08:24.145 --> 00:08:25.975
clump thickness of the breast tumor,

151
00:08:25.975 --> 00:08:28.280
uniformity of cell size of the tumor,

152
00:08:28.280 --> 00:08:30.325
uniformity of cell shape the tumor,

153
00:08:30.325 --> 00:08:33.195
and so on, and other features as well.

154
00:08:33.195 --> 00:08:36.830
It turns out one of the most interesting learning algorithms

155
00:08:36.830 --> 00:08:37.960
that we'll see in this course,

156
00:08:37.960 --> 00:08:41.700
as the learning algorithm that can deal with not just two,

157
00:08:41.700 --> 00:08:43.540
or three, or five features,

158
00:08:43.540 --> 00:08:45.745
but an infinite number of features.

159
00:08:45.745 --> 00:08:50.145
On this slide, I've listed a total of five different features.

160
00:08:50.145 --> 00:08:52.605
Two on the axis and three more up here.

161
00:08:52.605 --> 00:08:55.250
But it turns out that for some learning problems what you

162
00:08:55.250 --> 00:08:57.930
really want is not to use like three or five features,

163
00:08:57.930 --> 00:09:00.950
but instead you want to use an infinite number of features,

164
00:09:00.950 --> 00:09:02.675
an infinite number of attributes,

165
00:09:02.675 --> 00:09:05.610
so that your learning algorithm has lots of attributes,

166
00:09:05.610 --> 00:09:08.990
or features, or cues with which to make those predictions.

167
00:09:08.990 --> 00:09:12.755
So, how do you deal with an infinite number of features?

168
00:09:12.755 --> 00:09:15.500
How do you even store an infinite number of things in

169
00:09:15.500 --> 00:09:18.380
the computer when your computer is going to run out of memory?

170
00:09:18.380 --> 00:09:22.290
It turns out that when we talk about an algorithm called the Support Vector Machine,

171
00:09:22.290 --> 00:09:25.040
there will be a neat mathematical trick that will

172
00:09:25.040 --> 00:09:28.620
allow a computer to deal with an infinite number of features.

173
00:09:28.620 --> 00:09:33.590
Imagine that I didn't just write down two features here and three features on the right,

174
00:09:33.590 --> 00:09:36.440
but imagine that I wrote down an infinitely long list.

175
00:09:36.440 --> 00:09:38.540
I just kept writing more and more features,

176
00:09:38.540 --> 00:09:40.340
like an infinitely long list of features.

177
00:09:40.340 --> 00:09:44.695
It turns out we will come up with an algorithm that can deal with that.

178
00:09:44.695 --> 00:09:48.290
So, just to recap, in this course,

179
00:09:48.290 --> 00:09:50.115
we'll talk about Supervised Learning,

180
00:09:50.115 --> 00:09:52.750
and the idea is that in Supervised Learning,

181
00:09:52.750 --> 00:09:54.830
in every example in our data set,

182
00:09:54.830 --> 00:09:57.800
we are told what is the correct answer that

183
00:09:57.800 --> 00:10:01.115
we would have quite liked the algorithms have predicted on that example.

184
00:10:01.115 --> 00:10:02.930
Such as the price of the house,

185
00:10:02.930 --> 00:10:06.185
or whether a tumor is malignant or benign.

186
00:10:06.185 --> 00:10:09.230
We also talked about the regression problem,

187
00:10:09.230 --> 00:10:13.840
and by regression that means that our goal is to predict a continuous valued output.

188
00:10:13.840 --> 00:10:16.640
We talked about the classification problem where

189
00:10:16.640 --> 00:10:19.555
the goal is to predict a discrete value output.

190
00:10:19.555 --> 00:10:21.950
Just a quick wrap up question.

191
00:10:22.020 --> 00:10:25.520
Suppose you're running a company and you want to

192
00:10:25.520 --> 00:10:29.160
develop learning algorithms to address each of two problems.

193
00:10:29.160 --> 00:10:30.605
In the first problem,

194
00:10:30.605 --> 00:10:34.200
you have a large inventory of identical items.

195
00:10:34.200 --> 00:10:39.860
So, imagine that you have thousands of copies of some identical items to sell,

196
00:10:39.860 --> 00:10:44.025
and you want to predict how many of these items you sell over the next three months.

197
00:10:44.025 --> 00:10:46.470
In the second problem, problem two,

198
00:10:46.470 --> 00:10:49.390
you have lots of users,

199
00:10:49.390 --> 00:10:55.220
and you want to write software to examine each individual of your customer's accounts,

200
00:10:55.220 --> 00:10:57.075
so each one of your customer's accounts.

201
00:10:57.075 --> 00:11:02.485
For each account, decide whether or not the account has been hacked or compromised.

202
00:11:02.485 --> 00:11:04.615
So, for each of these problems,

203
00:11:04.615 --> 00:11:09.445
should they be treated as a classification problem or as a regression problem?

204
00:11:09.445 --> 00:11:10.960
When the video pauses,

205
00:11:10.960 --> 00:11:13.820
please use your mouse to select whichever of

206
00:11:13.820 --> 00:11:18.780
these four options on the left you think is the correct answer.

207
00:11:19.880 --> 00:11:22.870
So hopefully, you got that.

208
00:11:22.870 --> 00:11:24.235
This is the answer.

209
00:11:24.235 --> 00:11:26.480
For problem one, I would treat this as

210
00:11:26.480 --> 00:11:30.450
a regression problem because if I have thousands of items,

211
00:11:30.450 --> 00:11:33.220
well, I would probably just treat this as a real value,

212
00:11:33.220 --> 00:11:35.730
as a continuous value.

213
00:11:35.830 --> 00:11:41.340
Therefore, the number of items I sell as a continuous value.

214
00:11:41.340 --> 00:11:43.035
For the second problem,

215
00:11:43.035 --> 00:11:45.655
I would treat that as a classification problem,

216
00:11:45.655 --> 00:11:49.550
because I might say set the value I want to

217
00:11:49.550 --> 00:11:54.015
predict with zero to denote the account has not been hacked,

218
00:11:54.015 --> 00:11:59.310
and set the value one to denote an account that has been hacked into.

219
00:11:59.310 --> 00:12:02.930
So, just like your breast cancers where zero is benign, one is malignant.

220
00:12:02.930 --> 00:12:06.230
So, I might set this be zero or one depending on whether it's been hacked,

221
00:12:06.230 --> 00:12:12.340
and have an algorithm try to predict each one of these two discrete values.

222
00:12:12.340 --> 00:12:14.820
Because there's a small number of discrete values,

223
00:12:14.820 --> 00:12:18.000
I would therefore treat it as a classification problem.

224
00:12:18.000 --> 00:12:21.620
So, that's it for Supervised Learning.

225
00:12:21.620 --> 00:12:23.035
In the next video,

226
00:12:23.035 --> 00:12:25.200
I'll talk about Unsupervised Learning,

227
00:12:25.200 --> 00:12:29.320
which is the other major category of learning algorithm.