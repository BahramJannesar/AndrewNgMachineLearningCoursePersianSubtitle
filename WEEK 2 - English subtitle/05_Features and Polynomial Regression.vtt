WEBVTT

1
00:00:00.200 --> 00:00:03.878
You now know about linear regression with multiple variables.

2
00:00:03.910 --> 00:00:05.185
In this video, I wanna tell

3
00:00:05.185 --> 00:00:06.369
you a bit about the choice

4
00:00:06.380 --> 00:00:07.830
of features that you have and

5
00:00:07.830 --> 00:00:09.742
how you can get different learning

6
00:00:09.750 --> 00:00:11.477
algorithm, sometimes very powerful

7
00:00:11.477 --> 00:00:13.803
ones by choosing appropriate features.

8
00:00:13.810 --> 00:00:15.229
And in particular I also want

9
00:00:15.229 --> 00:00:17.826
to tell you about polynomial regression allows

10
00:00:17.826 --> 00:00:19.535
you to use the machinery of

11
00:00:19.535 --> 00:00:21.247
linear regression to fit very

12
00:00:21.247 --> 00:00:25.060
complicated, even very non-linear functions.

13
00:00:25.690 --> 00:00:28.827
Let's take the example of predicting the price of the house.

14
00:00:29.300 --> 00:00:31.147
Suppose you have two features,

15
00:00:31.147 --> 00:00:33.805
the frontage of house and the depth of the house.

16
00:00:33.805 --> 00:00:35.428
So, here's the picture of the house we're trying to sell.

17
00:00:35.428 --> 00:00:37.264
So, the frontage is

18
00:00:37.264 --> 00:00:40.103
defined as this distance

19
00:00:40.103 --> 00:00:43.009
is basically the width

20
00:00:43.009 --> 00:00:44.949
or the length of

21
00:00:44.960 --> 00:00:46.652
how wide your lot

22
00:00:46.652 --> 00:00:47.994
is if this that you

23
00:00:48.020 --> 00:00:49.468
own, and the depth

24
00:00:49.500 --> 00:00:53.120
of the house is how

25
00:00:53.130 --> 00:00:54.758
deep your property is, so

26
00:00:54.770 --> 00:00:57.992
there's a frontage, there's a depth.

27
00:00:57.992 --> 00:00:59.858
called frontage and depth.

28
00:00:59.858 --> 00:01:01.355
You might build a linear regression

29
00:01:01.360 --> 00:01:04.163
model like this where frontage

30
00:01:04.180 --> 00:01:06.062
is your first feature x1 and

31
00:01:06.062 --> 00:01:07.535
and depth is your second

32
00:01:07.535 --> 00:01:10.169
feature x2, but when you're

33
00:01:10.169 --> 00:01:11.772
applying linear regression, you don't

34
00:01:11.772 --> 00:01:13.342
necessarily have to use

35
00:01:13.342 --> 00:01:16.607
just the features x1 and x2 that you're given.

36
00:01:16.610 --> 00:01:20.531
What you can do is actually create new features by yourself.

37
00:01:20.531 --> 00:01:21.709
So, if I want to predict

38
00:01:21.710 --> 00:01:22.895
the price of a house, what I

39
00:01:22.895 --> 00:01:24.840
might do instead is decide

40
00:01:24.850 --> 00:01:27.468
that what really determines

41
00:01:27.490 --> 00:01:29.133
the size of the house is

42
00:01:29.133 --> 00:01:32.164
the area or the land area that I own.

43
00:01:32.190 --> 00:01:33.365
So, I might create a new feature.

44
00:01:33.380 --> 00:01:34.609
I'm just gonna call this feature

45
00:01:34.609 --> 00:01:40.409
x which is frontage, times depth.

46
00:01:40.440 --> 00:01:42.404
This is a multiplication symbol.

47
00:01:42.404 --> 00:01:44.334
It's a frontage x depth because

48
00:01:44.334 --> 00:01:46.040
this is the land area

49
00:01:46.090 --> 00:01:48.035
that I own and I might

50
00:01:48.035 --> 00:01:50.651
then select my hypothesis

51
00:01:50.710 --> 00:01:53.327
as that using just

52
00:01:53.350 --> 00:01:54.785
one feature which is my

53
00:01:54.785 --> 00:01:57.430
land area, right?

54
00:01:57.580 --> 00:01:58.939
Because the area of a

55
00:01:58.940 --> 00:02:00.345
rectangle is you know,

56
00:02:00.345 --> 00:02:01.432
the product of the length

57
00:02:01.460 --> 00:02:03.822
of the size So, depending

58
00:02:03.822 --> 00:02:05.253
on what insight you might have

59
00:02:05.280 --> 00:02:07.481
into a particular problem, rather than

60
00:02:07.490 --> 00:02:09.604
just taking the features [xx]

61
00:02:09.620 --> 00:02:11.103
that we happen to have started

62
00:02:11.130 --> 00:02:13.489
off with, sometimes by defining

63
00:02:13.489 --> 00:02:16.771
new features you might actually get a better model.

64
00:02:16.790 --> 00:02:18.163
Closely related to the

65
00:02:18.163 --> 00:02:19.745
idea of choosing your features

66
00:02:19.745 --> 00:02:22.973
is this idea called polynomial regression.

67
00:02:23.010 --> 00:02:26.868
Let's say you have a housing price data set that looks like this.

68
00:02:26.880 --> 00:02:29.646
Then there are a few different models you might fit to this.

69
00:02:29.660 --> 00:02:32.587
One thing you could do is fit a quadratic model like this.

70
00:02:32.600 --> 00:02:35.598
It doesn't look like a straight line fits this data very well.

71
00:02:35.598 --> 00:02:36.788
So maybe you want to fit

72
00:02:36.788 --> 00:02:38.408
a quadratic model like this

73
00:02:38.420 --> 00:02:40.248
where you think the size, where

74
00:02:40.248 --> 00:02:42.017
you think the price is a quadratic

75
00:02:42.020 --> 00:02:43.956
function and maybe that'll

76
00:02:43.970 --> 00:02:45.018
give you, you know, a fit

77
00:02:45.020 --> 00:02:47.070
to the data that looks like that.

78
00:02:47.280 --> 00:02:48.560
But then you may decide that your

79
00:02:48.570 --> 00:02:50.013
quadratic model doesn't make sense

80
00:02:50.013 --> 00:02:52.582
because of a quadratic function, eventually

81
00:02:52.582 --> 00:02:53.858
this function comes back down

82
00:02:53.858 --> 00:02:55.591
and well, we don't think housing

83
00:02:55.600 --> 00:02:58.899
prices should go down when the size goes up too high.

84
00:02:58.970 --> 00:03:00.649
So then maybe we might

85
00:03:00.650 --> 00:03:02.700
choose a different polynomial model

86
00:03:02.700 --> 00:03:04.274
and choose to use instead a

87
00:03:04.290 --> 00:03:07.480
cubic function, and where

88
00:03:07.480 --> 00:03:09.225
we have now a third-order term

89
00:03:09.225 --> 00:03:10.764
and we fit that, maybe

90
00:03:10.800 --> 00:03:12.367
we get this sort of

91
00:03:12.390 --> 00:03:13.907
model, and maybe the

92
00:03:13.910 --> 00:03:15.278
green line is a somewhat better fit

93
00:03:15.278 --> 00:03:18.052
to the data cause it doesn't eventually come back down.

94
00:03:18.052 --> 00:03:21.992
So how do we actually fit a model like this to our data?

95
00:03:22.020 --> 00:03:23.868
Using the machinery of multivariant

96
00:03:23.868 --> 00:03:27.059
linear regression, we can

97
00:03:27.059 --> 00:03:30.692
do this with a pretty simple modification to our algorithm.

98
00:03:30.692 --> 00:03:32.632
The form of the hypothesis we,

99
00:03:32.632 --> 00:03:34.217
we know how the fit

100
00:03:34.217 --> 00:03:35.782
looks like this, where we say

101
00:03:35.782 --> 00:03:37.612
H of x is theta zero

102
00:03:37.612 --> 00:03:41.608
plus theta one x one plus x two theta X3.

103
00:03:41.608 --> 00:03:42.775
And if we want to

104
00:03:42.775 --> 00:03:45.220
fit this cubic model that

105
00:03:45.250 --> 00:03:47.239
I have boxed in green,

106
00:03:47.239 --> 00:03:48.940
what we're saying is that

107
00:03:48.940 --> 00:03:49.825
to predict the price of a

108
00:03:49.825 --> 00:03:51.364
house, it's theta 0 plus theta

109
00:03:51.364 --> 00:03:53.056
1 times the size of the house

110
00:03:53.056 --> 00:03:55.905
plus theta 2 times the square size of the house.

111
00:03:55.910 --> 00:03:58.974
So this term is equal to that term.

112
00:03:58.974 --> 00:04:00.885
And then plus theta 3

113
00:04:00.890 --> 00:04:02.343
times the cube of the

114
00:04:02.350 --> 00:04:05.302
size of the house raises that third term.

115
00:04:05.470 --> 00:04:06.967
In order to map these

116
00:04:06.990 --> 00:04:08.668
two definitions to each other,

117
00:04:08.668 --> 00:04:10.339
well, the natural way

118
00:04:10.339 --> 00:04:12.128
to do that is to set

119
00:04:12.150 --> 00:04:13.568
the first feature x one to

120
00:04:13.568 --> 00:04:15.320
be the size of the house, and

121
00:04:15.320 --> 00:04:16.721
set the second feature x two

122
00:04:16.721 --> 00:04:17.766
to be the square of the size

123
00:04:17.766 --> 00:04:20.400
of the house, and set the third feature x three to

124
00:04:20.400 --> 00:04:22.780
be the cube of the size of the house.

125
00:04:22.800 --> 00:04:24.292
And, just by choosing my

126
00:04:24.292 --> 00:04:26.311
three features this way and

127
00:04:26.311 --> 00:04:27.720
applying the machinery of linear

128
00:04:27.720 --> 00:04:30.540
regression, I can fit this

129
00:04:30.540 --> 00:04:31.901
model and end up with

130
00:04:31.901 --> 00:04:34.374
a cubic fit to my data.

131
00:04:34.374 --> 00:04:35.523
I just want to point out one

132
00:04:35.523 --> 00:04:36.799
more thing, which is that

133
00:04:36.800 --> 00:04:38.610
if you choose your features

134
00:04:38.610 --> 00:04:40.925
like this, then feature scaling

135
00:04:40.925 --> 00:04:43.688
becomes increasingly important.

136
00:04:44.130 --> 00:04:45.254
So if the size of the

137
00:04:45.254 --> 00:04:46.794
house ranges from one to

138
00:04:46.800 --> 00:04:47.992
a thousand, so, you know,

139
00:04:47.992 --> 00:04:49.300
from one to a thousand square

140
00:04:49.310 --> 00:04:50.918
feet, say, then the size

141
00:04:50.930 --> 00:04:52.175
squared of the house will

142
00:04:52.175 --> 00:04:54.519
range from one to one

143
00:04:54.520 --> 00:04:55.953
million, the square of

144
00:04:55.953 --> 00:04:58.468
a thousand, and your third

145
00:04:58.490 --> 00:05:01.335
feature x cubed, excuse me

146
00:05:01.360 --> 00:05:03.106
you, your third feature x

147
00:05:03.120 --> 00:05:04.732
three, which is the size

148
00:05:04.732 --> 00:05:05.941
cubed of the house, will range

149
00:05:05.950 --> 00:05:07.478
from one two ten to

150
00:05:07.478 --> 00:05:09.311
the nine, and so these

151
00:05:09.330 --> 00:05:10.955
three features take on very

152
00:05:10.955 --> 00:05:13.459
different ranges of values, and

153
00:05:13.490 --> 00:05:15.105
it's important to apply feature

154
00:05:15.110 --> 00:05:16.509
scaling if you're using gradient

155
00:05:16.509 --> 00:05:18.554
descent to get them into

156
00:05:18.554 --> 00:05:21.139
comparable ranges of values.

157
00:05:21.140 --> 00:05:23.243
Finally, here's one last example

158
00:05:23.250 --> 00:05:25.138
of how you really have

159
00:05:25.150 --> 00:05:29.056
broad choices in the features you use.

160
00:05:29.090 --> 00:05:30.446
Earlier we talked about how a

161
00:05:30.446 --> 00:05:31.559
quadratic model like this might

162
00:05:31.559 --> 00:05:33.122
not be ideal because, you know,

163
00:05:33.122 --> 00:05:34.408
maybe a quadratic model fits the

164
00:05:34.408 --> 00:05:35.952
data okay, but the quadratic

165
00:05:35.952 --> 00:05:37.514
function goes back down

166
00:05:37.514 --> 00:05:39.065
and we really don't want, right,

167
00:05:39.070 --> 00:05:40.352
housing prices that go down,

168
00:05:40.352 --> 00:05:43.567
to predict that, as the size of housing freezes.

169
00:05:43.567 --> 00:05:45.388
But rather than going to

170
00:05:45.388 --> 00:05:46.938
a cubic model there, you

171
00:05:46.938 --> 00:05:48.389
have, maybe, other choices of

172
00:05:48.389 --> 00:05:50.798
features and there are many possible choices.

173
00:05:50.800 --> 00:05:52.313
But just to give you another

174
00:05:52.313 --> 00:05:53.691
example of a reasonable

175
00:05:53.691 --> 00:05:55.620
choice, another reasonable choice

176
00:05:55.620 --> 00:05:57.263
might be to say that the

177
00:05:57.263 --> 00:05:58.832
price of a house is theta

178
00:05:58.850 --> 00:05:59.992
zero plus theta one times

179
00:05:59.992 --> 00:06:01.264
the size, and then plus theta

180
00:06:01.320 --> 00:06:03.625
two times the square root of the size, right?

181
00:06:03.630 --> 00:06:05.364
So the square root function is

182
00:06:05.364 --> 00:06:08.110
this sort of function, and maybe

183
00:06:08.110 --> 00:06:09.318
there will be some value of theta

184
00:06:09.318 --> 00:06:11.355
one, theta two, theta three, that

185
00:06:11.355 --> 00:06:14.049
will let you take this model

186
00:06:14.080 --> 00:06:15.445
and, for the curve that looks

187
00:06:15.445 --> 00:06:16.952
like that, and, you know,

188
00:06:16.952 --> 00:06:19.500
goes up, but sort of flattens

189
00:06:19.520 --> 00:06:21.529
out a bit and doesn't ever

190
00:06:21.540 --> 00:06:23.877
come back down.

191
00:06:24.154 --> 00:06:26.584
And, so, by having insight into, in

192
00:06:26.584 --> 00:06:27.630
this case, the shape of a

193
00:06:27.630 --> 00:06:30.952
square root function, and, into

194
00:06:30.990 --> 00:06:32.555
the shape of the data, by choosing

195
00:06:32.555 --> 00:06:36.469
different features, you can sometimes get better models.

196
00:06:36.469 --> 00:06:39.026
In this video, we talked about polynomial regression.

197
00:06:39.026 --> 00:06:40.672
That is, how to fit a

198
00:06:40.672 --> 00:06:42.298
polynomial, like a quadratic function,

199
00:06:42.298 --> 00:06:43.868
or a cubic function, to your data.

200
00:06:43.868 --> 00:06:45.112
Was also throw out this idea,

201
00:06:45.112 --> 00:06:46.640
that you have a choice in what

202
00:06:46.640 --> 00:06:47.732
features to use, such as

203
00:06:47.748 --> 00:06:48.804
that instead of using

204
00:06:48.804 --> 00:06:50.078
the frontish and the depth

205
00:06:50.078 --> 00:06:51.092
of the house, maybe, you can

206
00:06:51.092 --> 00:06:53.133
multiply them together to get

207
00:06:53.133 --> 00:06:55.317
a feature that captures the land area of a house.

208
00:06:55.317 --> 00:06:57.551
In case this seems a little

209
00:06:57.551 --> 00:06:58.895
bit bewildering, that with all

210
00:06:58.896 --> 00:07:03.265
these different feature choices, so how do I decide what features to use.

211
00:07:03.265 --> 00:07:04.594
Later in this class, we'll talk

212
00:07:04.594 --> 00:07:06.622
about some algorithms were automatically

213
00:07:06.622 --> 00:07:08.083
choosing what features are used,

214
00:07:08.083 --> 00:07:09.466
so you can have an

215
00:07:09.466 --> 00:07:10.611
algorithm look at the data

216
00:07:10.611 --> 00:07:12.040
and automatically choose for you

217
00:07:12.040 --> 00:07:13.357
whether you want to fit a

218
00:07:13.357 --> 00:07:15.528
quadratic function, or a cubic function, or something else.

219
00:07:15.528 --> 00:07:17.164
But, until we get to

220
00:07:17.164 --> 00:07:18.764
those algorithms now I just

221
00:07:18.764 --> 00:07:20.295
want you to be aware that

222
00:07:20.295 --> 00:07:21.582
you have a choice in

223
00:07:21.582 --> 00:07:23.094
what features to use, and

224
00:07:23.094 --> 00:07:25.256
by designing different features

225
00:07:25.256 --> 00:07:26.888
you can fit more complex functions

226
00:07:26.888 --> 00:07:28.156
your data then just fitting a

227
00:07:28.156 --> 00:07:30.471
straight line to the data and

228
00:07:30.471 --> 00:07:32.092
in particular you can put polynomial

229
00:07:32.092 --> 00:07:35.065
functions as well and sometimes

230
00:07:35.065 --> 00:07:36.072
by appropriate insight into the

231
00:07:36.072 --> 00:07:37.564
feature simply get a much

232
00:07:37.564 --> 00:07:40.020
better model for your data.