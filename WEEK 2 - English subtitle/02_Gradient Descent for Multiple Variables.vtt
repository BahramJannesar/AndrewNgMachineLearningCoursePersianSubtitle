WEBVTT

1
00:00:00.220 --> 00:00:03.688
In the previous video, we talked about
the form of the hypothesis for linear

2
00:00:03.688 --> 00:00:07.246
regression with multiple features
or with multiple variables.

3
00:00:07.246 --> 00:00:11.912
In this video, let's talk about how to
fit the parameters of that hypothesis.

4
00:00:11.912 --> 00:00:15.175
In particular let's talk about how
to use gradient descent for linear

5
00:00:15.175 --> 00:00:19.875
regression with multiple features.

6
00:00:19.875 --> 00:00:24.802
To quickly summarize our notation,
this is our formal hypothesis in

7
00:00:24.802 --> 00:00:31.509
multivariable linear regression where
we've adopted the convention that x0=1.

8
00:00:31.509 --> 00:00:37.505
The parameters of this model are theta0
through theta n, but instead of thinking

9
00:00:37.505 --> 00:00:42.385
of this as n separate parameters, which
is valid, I'm instead going to think of

10
00:00:42.385 --> 00:00:51.175
the parameters as theta where theta
here is a n+1-dimensional vector.

11
00:00:51.175 --> 00:00:55.498
So I'm just going to think of the
parameters of this model

12
00:00:55.498 --> 00:00:58.674
as itself being a vector.

13
00:00:58.674 --> 00:01:03.507
Our cost function is J of theta0 through
theta n which is given by this usual

14
00:01:03.507 --> 00:01:08.983
sum of square of error term. But again
instead of thinking of J as a function

15
00:01:08.983 --> 00:01:14.016
of these n+1 numbers, I'm going to
more commonly write J as just a

16
00:01:14.016 --> 00:01:22.275
function of the parameter vector theta
so that theta here is a vector.

17
00:01:22.275 --> 00:01:26.897
Here's what gradient descent looks like.
We're going to repeatedly update each

18
00:01:26.897 --> 00:01:32.142
parameter theta j according to theta j
minus alpha times this derivative term.

19
00:01:32.142 --> 00:01:37.868
And once again we just write this as
J of theta, so theta j is updated as

20
00:01:37.868 --> 00:01:41.840
theta j minus the learning rate
alpha times the derivative, a partial

21
00:01:41.840 --> 00:01:47.840
derivative of the cost function with
respect to the parameter theta j.

22
00:01:47.840 --> 00:01:51.305
Let's see what this looks like when
we implement gradient descent and,

23
00:01:51.305 --> 00:01:55.985
in particular, let's go see what that
partial derivative term looks like.

24
00:01:55.985 --> 00:02:01.383
Here's what we have for gradient descent
for the case of when we had N=1 feature.

25
00:02:01.383 --> 00:02:06.782
We had two separate update rules for
the parameters theta0 and theta1, and

26
00:02:06.782 --> 00:02:12.779
hopefully these look familiar to you.
And this term here was of course the

27
00:02:12.779 --> 00:02:17.672
partial derivative of the cost function
with respect to the parameter of theta0,

28
00:02:17.672 --> 00:02:21.891
and similarly we had a different
update rule for the parameter theta1.

29
00:02:21.891 --> 00:02:26.259
There's one little difference which is
that when we previously had only one

30
00:02:26.259 --> 00:02:31.992
feature, we would call that feature x(i)
but now in our new notation

31
00:02:31.992 --> 00:02:38.462
we would of course call this 
x(i)<u>1 to denote our one feature.</u>

32
00:02:38.462 --> 00:02:41.019
So that was for when
we had only one feature.

33
00:02:41.019 --> 00:02:44.496
Let's look at the new algorithm for
we have more than one feature,

34
00:02:44.496 --> 00:02:47.350
where the number of features n
may be much larger than one.

35
00:02:47.350 --> 00:02:53.158
We get this update rule for gradient
descent and, maybe for those of you that

36
00:02:53.158 --> 00:02:57.781
know calculus, if you take the
definition of the cost function and take

37
00:02:57.781 --> 00:03:03.312
the partial derivative of the cost
function J with respect to the parameter

38
00:03:03.312 --> 00:03:08.119
theta j, you'll find that that partial
derivative is exactly that term that

39
00:03:08.119 --> 00:03:10.665
I've drawn the blue box around.

40
00:03:10.665 --> 00:03:14.837
And if you implement this you will
get a working implementation of

41
00:03:14.837 --> 00:03:18.962
gradient descent for
multivariate linear regression.

42
00:03:18.962 --> 00:03:21.572
The last thing I want to do on
this slide is give you a sense of

43
00:03:21.572 --> 00:03:26.882
why these new and old algorithms are
sort of the same thing or why they're

44
00:03:26.882 --> 00:03:30.904
both similar algorithms or why they're
both gradient descent algorithms.

45
00:03:30.904 --> 00:03:34.363
Let's consider a case
where we have two features

46
00:03:34.363 --> 00:03:37.488
or maybe more than two features,
so we have three update rules for

47
00:03:37.488 --> 00:03:42.680
the parameters theta0, theta1, theta2
and maybe other values of theta as well.

48
00:03:42.680 --> 00:03:49.457
If you look at the update rule for
theta0, what you find is that this

49
00:03:49.457 --> 00:03:55.300
update rule here is the same as
the update rule that we had previously

50
00:03:55.300 --> 00:03:57.350
for the case of n = 1.

51
00:03:57.350 --> 00:04:00.203
And the reason that they are
equivalent is, of course,

52
00:04:00.203 --> 00:04:06.871
because in our notational convention we
had this x(i)<u>0 = 1 convention, which is</u>

53
00:04:06.871 --> 00:04:12.003
why these two term that I've drawn the
magenta boxes around are equivalent.

54
00:04:12.003 --> 00:04:16.010
Similarly, if you look the update
rule for theta1, you find that

55
00:04:16.010 --> 00:04:21.540
this term here is equivalent to
the term we previously had,

56
00:04:21.540 --> 00:04:25.020
or the equation or the update
rule we previously had for theta1,

57
00:04:25.020 --> 00:04:30.222
where of course we're just using
this new notation x(i)<u>1 to denote</u>

58
00:04:30.222 --> 00:04:37.605
our first feature, and now that we have
more than one feature we can have

59
00:04:37.605 --> 00:04:43.560
similar update rules for the other
parameters like theta2 and so on.

60
00:04:43.560 --> 00:04:48.219
There's a lot going on on this slide
so I definitely encourage you

61
00:04:48.219 --> 00:04:52.020
if you need to to pause the video
and look at all the math on this slide

62
00:04:52.020 --> 00:04:55.446
slowly to make sure you understand
everything that's going on here.

63
00:04:55.446 --> 00:05:00.440
But if you implement the algorithm
written up here then you have

64
00:05:00.440 --> 00:05:51.300
a working implementation of linear
regression with multiple features.